{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open('../data/cleaned_wiki.json', 'r') as f:\n",
    "    data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The title for paragraph ID 41 is: program exampl\n"
     ]
    }
   ],
   "source": [
    "#fonction to get the title of the paragraph\n",
    "def get_title_by_id(paragraph_id, data):\n",
    "    for section in data:\n",
    "        for content_item in section[\"content\"]:\n",
    "            if content_item[\"type\"].startswith(\"h\") and content_item[\"ids\"]:\n",
    "                if paragraph_id in content_item[\"ids\"]:\n",
    "                    return content_item[\"title\"]\n",
    "    return None\n",
    "\n",
    "\n",
    "\n",
    "#test \n",
    "paragraph_id_to_find = 41  \n",
    "title = get_title_by_id(paragraph_id_to_find, data)\n",
    "\n",
    "if title:\n",
    "    print(f\"The title for paragraph ID {paragraph_id_to_find} is: {title}\")\n",
    "else:\n",
    "    print(f\"No title found for paragraph ID {paragraph_id_to_find}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fonction to get all the title related to the paragraph\n",
    "\n",
    "def get_titles_by_id(paragraph_id, data):\n",
    "\n",
    "    #set values\n",
    "    page_title = data[0][\"title\"]  #the title of the page\n",
    "    previous_h2_title = None #the title of the division containing subtitles\n",
    "    paragraph_title = None #the title of the paragraph\n",
    "\n",
    "    for section in data:\n",
    "        for content_item in section[\"content\"]:\n",
    "            #verify the type of the title to check if it is a sub title\n",
    "            if content_item[\"type\"] == \"h2\":\n",
    "                previous_h2_title = content_item[\"title\"]\n",
    "            elif content_item[\"type\"] == \"h3\" and content_item[\"ids\"] and paragraph_id in content_item[\"ids\"]:\n",
    "                paragraph_title = content_item[\"title\"]\n",
    "                return page_title, paragraph_title, previous_h2_title\n",
    "\n",
    "    return page_title, paragraph_title, previous_h2_title\n",
    "\n",
    "# test\n",
    "paragraph_id_to_find = 41  \n",
    "page_title, paragraph_title, previous_h2_title = get_titles_by_id(paragraph_id_to_find, data)\n",
    "\n",
    "if paragraph_title:\n",
    "    print(f\"Page Title: {page_title}\")\n",
    "    print(f\"Paragraph Title: {paragraph_title}\")\n",
    "    if previous_h2_title:\n",
    "        print(f\"Previous H2 Title: {previous_h2_title}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "capit franc\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\jules\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "import json\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "STOPWORDS = set(stopwords.words('english'))\n",
    "\n",
    "def clean_text(text: str) -> str:\n",
    "    # convert to lowercase\n",
    "    lower_cased = text.lower()\n",
    "    # remove references in square brackets\n",
    "    no_references = re.sub(r'\\[.*?\\]', '', lower_cased)\n",
    "    # keep only alphanumeric characters\n",
    "    alphanumeric = re.sub(r'\\W+', ' ', no_references)\n",
    "    # remove strange unicode characters\n",
    "    alphanumeric = re.sub(r'[^\\x00-\\x7F]+', '', alphanumeric)\n",
    "    # remove stopwords\n",
    "    no_stopwords = ' '.join([word for word in alphanumeric.split() if word not in STOPWORDS])\n",
    "    # stemmize\n",
    "    stemmed = ' '.join([PorterStemmer().stem(word) for word in no_stopwords.split()])\n",
    "    return stemmed\n",
    "\n",
    "def get_n_gram(n, text):\n",
    "    # get word n_gram\n",
    "    words = text.split()\n",
    "    n_gram = []\n",
    "    for i in range(len(words) - n + 1):\n",
    "        n_gram.append(' '.join(words[i:i+n]))\n",
    "    return n_gram\n",
    "\n",
    "query = 'What is the capital of France?'\n",
    "\n",
    "cleaned_query = clean_text(query)\n",
    "print(cleaned_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_paragraph_by_query(query, paragraph_id, json_data):\n",
    "    page_title, paragraph_title, previous_h2_title = get_titles_by_id(paragraph_id, json_data)\n",
    "\n",
    "    if paragraph_title:\n",
    "\n",
    "        score = 0\n",
    "\n",
    "        # Check if any word from the query is present in the titles\n",
    "\n",
    "        #split the query\n",
    "        query_words = query.lower().split()\n",
    "\n",
    "        for word in query_words:\n",
    "            if word in page_title.lower():\n",
    "                score += 1  \n",
    "            elif word in paragraph_title.lower():\n",
    "                score += 100 # Add higher score for matching in page title\n",
    "            elif previous_h2_title and word in previous_h2_title.lower():\n",
    "                score += 10\n",
    "\n",
    "        return page_title, paragraph_title, previous_h2_title, score\n",
    "\n",
    "    return None\n",
    "\n",
    "# Test\n",
    "query_to_check = \"Python programming language\" \n",
    "paragraph_id_to_find = 10  \n",
    "result = score_paragraph_by_query(query_to_check, paragraph_id_to_find, data)\n",
    "\n",
    "\n",
    "if result:\n",
    "    page_title, paragraph_title, previous_h2_title, score = result\n",
    "    print(f\"Page Title: {page_title}\")\n",
    "    print(f\"Paragraph Title: {paragraph_title}\")\n",
    "    if previous_h2_title:\n",
    "        print(f\"Previous H2 Title: {previous_h2_title}\")\n",
    "    print(f\"Score: {score}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unhashable type: 'list'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 44\u001b[0m\n\u001b[0;32m     42\u001b[0m \u001b[38;5;66;03m# Example usage:\u001b[39;00m\n\u001b[0;32m     43\u001b[0m query_to_rank \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mWhat is the capital of France?\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m---> 44\u001b[0m ranked_paragraph_scores \u001b[38;5;241m=\u001b[39m \u001b[43mrank_paragraphs_by_similarity\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery_to_rank\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     46\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRanked Paragraph Scores: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mranked_paragraph_scores\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[6], line 35\u001b[0m, in \u001b[0;36mrank_paragraphs_by_similarity\u001b[1;34m(query, json_data)\u001b[0m\n\u001b[0;32m     33\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, title \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(page_titles \u001b[38;5;241m+\u001b[39m previous_h2_titles \u001b[38;5;241m+\u001b[39m paragraph_titles):\n\u001b[0;32m     34\u001b[0m     score \u001b[38;5;241m=\u001b[39m calculate_score(cleaned_query, title)\n\u001b[1;32m---> 35\u001b[0m     \u001b[43mscores\u001b[49m\u001b[43m[\u001b[49m\u001b[43mparagraph_ids\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;241m=\u001b[39m score\n\u001b[0;32m     37\u001b[0m \u001b[38;5;66;03m# Sort the dictionary by scores in descending order\u001b[39;00m\n\u001b[0;32m     38\u001b[0m sorted_scores \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(\u001b[38;5;28msorted\u001b[39m(scores\u001b[38;5;241m.\u001b[39mitems(), key\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mlambda\u001b[39;00m item: item[\u001b[38;5;241m1\u001b[39m], reverse\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m))\n",
      "\u001b[1;31mTypeError\u001b[0m: unhashable type: 'list'"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "def calculate_score(query, title):\n",
    "    query_words = set(word_tokenize(clean_text(query)))\n",
    "    title_words = set(word_tokenize(clean_text(title)))\n",
    "    overlap = len(query_words.intersection(title_words))\n",
    "    return overlap\n",
    "\n",
    "#example\n",
    "# Query tokens: ['capit', 'franc']\n",
    "# Title tokens: ['capit', 'citi', 'europ', 'explor', 'pari']\n",
    "# Intersection: {'capit'}\n",
    "# Overlap score: 1\n",
    "\n",
    "def rank_paragraphs_by_similarity(query, json_data):\n",
    "    cleaned_query = clean_text(query)\n",
    "    page_titles = []\n",
    "    paragraph_titles = []\n",
    "    previous_h2_titles = []\n",
    "    paragraph_ids = []\n",
    "\n",
    "    # Extract titles and IDs from JSON\n",
    "    for section in json_data:\n",
    "        for content_item in section[\"content\"]:\n",
    "            if content_item[\"type\"] == \"h2\":\n",
    "                previous_h2_titles.append(content_item[\"title\"])\n",
    "            elif content_item[\"type\"] == \"h3\" and content_item[\"ids\"]:\n",
    "                paragraph_titles.append(content_item[\"title\"])\n",
    "                paragraph_ids.append(content_item[\"ids\"])\n",
    "\n",
    "    # Calculate word scores\n",
    "    scores = {}\n",
    "    for i, title in enumerate(page_titles + previous_h2_titles + paragraph_titles):\n",
    "        score = calculate_score(cleaned_query, title)\n",
    "        scores[paragraph_ids[i]] = score\n",
    "\n",
    "    # Sort the dictionary by scores in descending order\n",
    "    sorted_scores = dict(sorted(scores.items(), key=lambda item: item[1], reverse=True))\n",
    "\n",
    "    return sorted_scores\n",
    "\n",
    "# Example usage:\n",
    "query_to_rank = 'What is the capital of France?'\n",
    "ranked_paragraph_scores = rank_paragraphs_by_similarity(query_to_rank, data)\n",
    "\n",
    "print(f\"Ranked Paragraph Scores: {ranked_paragraph_scores}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
