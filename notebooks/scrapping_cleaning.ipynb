{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='toc1_'></a>[NLP Final Project - Wikipedia Search Engine](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Table of contents**<a id='toc0_'></a>    \n",
    "- [NLP Final Project - Wikipedia Search Engine](#toc1_)    \n",
    "  - [1 - Scrapping Wikipedia](#toc1_1_)    \n",
    "    - [Scrapping class](#toc1_1_1_)    \n",
    "    - [Saving the links](#toc1_1_2_)    \n",
    "    - [- Adding the ids of the paragraphs](#toc1_1_3_)    \n",
    "  - [2 - Cleaning the data](#toc1_2_)    \n",
    "  - [3 - Creating the n-grams](#toc1_3_)    \n",
    "\n",
    "<!-- vscode-jupyter-toc-config\n",
    "\tnumbering=false\n",
    "\tanchor=true\n",
    "\tflat=false\n",
    "\tminLevel=1\n",
    "\tmaxLevel=6\n",
    "\t/vscode-jupyter-toc-config -->\n",
    "<!-- THIS CELL WILL BE REPLACED ON TOC UPDATE. DO NOT WRITE YOUR TEXT IN THIS CELL -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All the main parts has been done individually and then merged together. The scrapping part has'nt been runned before saving this notebook as it takes a lot of time to run."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc1_1_'></a>[1 - Scrapping Wikipedia](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc1_1_1_'></a>[Scrapping class](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "import time\n",
    "\n",
    "class WikiPage():\n",
    "    \"\"\"\n",
    "    This class represents a Wikipedia page and provides methods to fetch and parse the page content.\n",
    "    \n",
    "    Attributes:\n",
    "        url: The URL of the Wikipedia page.\n",
    "        soup: A BeautifulSoup object representing the parsed HTML content of the page.\n",
    "        links: A list of Wikipedia links from the page content.\n",
    "        title: The title of the Wikipedia page.\n",
    "        content: The content of the Wikipedia page.\n",
    "        summary: The summary of the Wikipedia page.\n",
    "    \"\"\"\n",
    "    def __init__(self, url) -> None:\n",
    "        \"\"\"\n",
    "        Initializes a new instance of the WikiPage class with the specified URL.\n",
    "        \"\"\"\n",
    "        self.url = url\n",
    "        self.soup = self.get_soup()\n",
    "        self.links = self.get_links()\n",
    "        self.title = self.get_h1()\n",
    "        self.content = self.get_content()[1:]\n",
    "        self.summary = self.get_summary()\n",
    "\n",
    "    def get_wiki_page(self) -> str:\n",
    "        \"\"\"\n",
    "        Fetches the HTML content of the Wikipedia page.\n",
    "        Returns:\n",
    "            The HTML content of the page as a string.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            response = requests.get(self.url)\n",
    "            response.raise_for_status()\n",
    "            return response.text\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"Error fetching {self.url}: {e}\")\n",
    "            return None\n",
    "        \n",
    "    def get_soup(self) -> BeautifulSoup:\n",
    "        \"\"\"\n",
    "        Parses the HTML content of the Wikipedia page using BeautifulSoup.\n",
    "        Returns:\n",
    "            A BeautifulSoup object representing the parsed HTML.\n",
    "        \"\"\"\n",
    "        page = self.get_wiki_page()\n",
    "        soup = BeautifulSoup(page, \"html.parser\")\n",
    "        return soup\n",
    "    \n",
    "    def get_links(self) -> list:\n",
    "        \"\"\"\n",
    "        Retrieves a list of Wikipedia links from the page content.\n",
    "        Returns:\n",
    "            A list of Wikipedia links as strings.\n",
    "        \"\"\"\n",
    "        allLinks = self.soup.find(id=\"bodyContent\").find_all(\"a\")\n",
    "        links = [link[\"href\"] for link in allLinks\n",
    "                # check if it has an href attribute\n",
    "                if link.has_attr(\"href\") \n",
    "                # check if it is a wikipedia link\n",
    "                and link[\"href\"].startswith(\"/wiki/\")\n",
    "                # check if it is not a file \n",
    "                and not link[\"href\"].endswith((\".jpg\", \".png\", \".svg\"))\n",
    "                # check if it is not a special page\n",
    "                and \"Special:\" not in link[\"href\"]\n",
    "                # check if it is not a help page\n",
    "                and \"Help:\" not in link[\"href\"] \n",
    "                # check if it is not a wikipedia page\n",
    "                and \"Wikipedia:\" not in link[\"href\"]]\n",
    "        return links\n",
    "    \n",
    "    # Parser\n",
    "    def get_h1(self) -> str:\n",
    "        \"\"\"\n",
    "        Retrieves the title of the Wikipedia page.\n",
    "        Returns:\n",
    "            The title of the page as a string.\n",
    "        \"\"\"\n",
    "        return self.soup.find(id=\"firstHeading\").text\n",
    "    \n",
    "    def get_summary(self) -> list:\n",
    "        \"\"\"\n",
    "        Retrieves the summary of the Wikipedia page.\n",
    "        Returns:\n",
    "            A list of paragraphs representing the summary.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            if self.soup.find(\"table\"):\n",
    "                return [p.text for p in self.soup.find(\"table\").find_next_siblings(\"p\")]\n",
    "            else:\n",
    "                # if there is no table we must get the summary in another way\n",
    "                # get the first 10 paragraphs\n",
    "                summary = [p.text for p in self.soup.find_all(\"p\")[:10]]\n",
    "                # get the first paragraph of the content (that comes after the first h2)\n",
    "                first_p = self.content[0][\"paragraphs\"]\n",
    "                # return the summary that are not in the first paragraph\n",
    "                return [p for p in summary if p not in first_p]\n",
    "        except Exception as e:\n",
    "            print(f\"Error getting summary for {self.url}: {e}\")\n",
    "            return None\n",
    "    \n",
    "    def get_content(self) -> list:\n",
    "        \"\"\"\n",
    "        Retrieves the content of the Wikipedia page.\n",
    "        Returns:\n",
    "            A list of dictionaries representing the content sections.\n",
    "        \"\"\"\n",
    "        content = []\n",
    "        tags = [f\"h{n}\" for n in range(2, 4)]\n",
    "        # find all h2 or h3 tags\n",
    "        for tag in self.soup.find_all(tags):\n",
    "            paragraph = []\n",
    "            # find all siblings of the tag until we find a new heading tag\n",
    "            for sibling in tag.next_siblings:\n",
    "                if sibling.name in tags:\n",
    "                    break\n",
    "                if sibling.name == \"p\":\n",
    "                    paragraph.append(sibling.text)\n",
    "            content.append({\n",
    "                \"type\": tag.name,\n",
    "                \"title\": tag.text,\n",
    "                \"paragraphs\": paragraph\n",
    "            })\n",
    "        return content\n",
    "\n",
    "    def wiki_page_to_dict(self):\n",
    "        \"\"\"\n",
    "        Converts the WikiPage object to a dictionary.\n",
    "        Returns:\n",
    "            A dictionary representation of the WikiPage object.\n",
    "        \"\"\"\n",
    "        return {\n",
    "            \"url\": self.url,\n",
    "            \"title\": self.title,\n",
    "            \"summary\": self.summary,\n",
    "            \"content\": self.content,\n",
    "        }\n",
    "    \n",
    "    def to_json(self, filename=\"data/raw_wiki.json\"):\n",
    "        \"\"\"\n",
    "        Serializes the WikiPage object to JSON and appends it to a file.\n",
    "        Args:\n",
    "            filename: The path to the file to append the JSON data to.\n",
    "        \"\"\"\n",
    "        with open(filename, \"r\") as f:\n",
    "            data = json.load(f)\n",
    "            data.append(self.wiki_page_to_dict())\n",
    "        with open(filename, \"w\") as f:\n",
    "            json.dump(data, f, indent=4)\n",
    "\n",
    "        \n",
    "\n",
    "    def links_to_json(self, filename=\"data/links.json\"):\n",
    "        \"\"\"\n",
    "        Serializes the links of the WikiPage object to JSON and appends it to a file.\n",
    "        Args:\n",
    "            filename: The path to the file to append the JSON data to.\n",
    "        \"\"\"\n",
    "        with open(filename, \"r\") as f:\n",
    "            data = json.load(f)\n",
    "            data.append({\n",
    "                \"url\": self.url,\n",
    "                \"links\": self.links\n",
    "            })\n",
    "        with open(filename, \"w\") as f:\n",
    "            json.dump(data, f, indent=4)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "print(\"Starting scrapping...\")\n",
    "\n",
    "# create the root page\n",
    "root_url = \"https://en.wikipedia.org/wiki/Python_(programming_language)\"\n",
    "root_page = WikiPage(root_url)\n",
    "# write the root page to the json file\n",
    "# with open(\"data/raw_wiki.json\", \"w\") as f:\n",
    "#     json.dump([root_page.wiki_page_to_dict()], f, indent=4)\n",
    "# # write the root page links to the json file\n",
    "# with open(\"data/links.json\", \"w\") as f:\n",
    "#     json.dump([{\n",
    "#         \"url\": root_url,\n",
    "#         \"links\": root_page.links\n",
    "#     }], f, indent=4)\n",
    "done = [root_url]\n",
    "fifo_links = root_page.links\n",
    "\n",
    "# while we don't have 5000 pages in the json file we continue\n",
    "while len(done) < 5000:\n",
    "    link = fifo_links.pop(0)\n",
    "    try:\n",
    "        link_url = \"https://en.wikipedia.org\" + link\n",
    "        if link_url in done:\n",
    "            continue\n",
    "        # create the page\n",
    "        page = WikiPage(link_url)\n",
    "        # write the page to the json file\n",
    "        page.to_json()\n",
    "        # write the page links to the json file\n",
    "        page.links_to_json()\n",
    "        # add the page to the done list\n",
    "        done.append(page.url)\n",
    "        if len(fifo_links) + len(done) < 5000:\n",
    "            # check the length of the total links to make sure we don't have too much links\n",
    "            fifo_links = fifo_links + page.links\n",
    "    except Exception as e:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc1_1_2_'></a>[Saving the links](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we have scrapped also the links each article was redirecting to, we can now create a ranking of the most cited articles inside our dataset. We will do this by creating a dictionary with the number of times each article was cited. We will then sort the dictionary by the number of citations. It could be usefeul for our PageRank algorithm to have a list of the most cited articles, as they are probably the most important ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the links\n",
    "with open(\"data/links.json\", \"r\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# create a new dict with the links as keys and the values as the number of times they appear\n",
    "links = {}\n",
    "for item in data:\n",
    "    for link in item[\"links\"]:\n",
    "        if link in links:\n",
    "            links[link] += 1\n",
    "        else:\n",
    "            links[link] = 1\n",
    "\n",
    "# sort the dict by the number of times they appear\n",
    "sorted_links = {k: v for k, v in sorted(links.items(), key=lambda item: item[1], reverse=True)}\n",
    "\n",
    "# save the sorted links\n",
    "with open(\"data/sorted_links.json\", \"w\") as f:\n",
    "    json.dump(sorted_links, f, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The links could help us in the future in order to make a better PageRank score for the pages that are the most cited in the dataset. The idea is to give more importance to the articles that are well referenced. This is how the scientific papers are ranked, the more a paper is cited, the more important it is considered."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc1_1_3_'></a>[- Adding the ids of the paragraphs](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "During the scrapping part, we forgot to add the ids of the paragraphs sequentially. We will now add them to the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open('../data/raw_wiki.json', 'r') as f:\n",
    "    data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add the ids of the paragraphs\n",
    "index = 0\n",
    "for page in data:\n",
    "    if page['summary']:\n",
    "        # add the field summary_ids in 4th level\n",
    "        page['summary_ids'] = []\n",
    "        for paragraph in page['summary']:\n",
    "            # we append the index to the summary_ids\n",
    "            page['summary_ids'].append(index)\n",
    "            index += 1\n",
    "    for section in page['content']:\n",
    "        # add the field ids in 4th level\n",
    "        section['ids'] = []\n",
    "        for paragraph in section['paragraphs']:\n",
    "            # we append the index to the ids\n",
    "            section['ids'].append(index)\n",
    "            index += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the data with the ids\n",
    "with open('../data/raw_wiki_with_ids.json', 'w') as f:\n",
    "    json.dump(data, f, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc1_2_'></a>[2 - Cleaning the data](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "import json\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "STOPWORDS = set(stopwords.words('english'))\n",
    "\n",
    "def clean_text(text: str) -> str:\n",
    "    # convert to lowercase\n",
    "    lower_cased = text.lower()\n",
    "    # remove references in square brackets\n",
    "    no_references = re.sub(r'\\[.*?\\]', '', lower_cased)\n",
    "    # keep only alphanumeric characters\n",
    "    alphanumeric = re.sub(r'\\W+', ' ', no_references)\n",
    "    # remove strange unicode characters\n",
    "    alphanumeric = re.sub(r'[^\\x00-\\x7F]+', '', alphanumeric)\n",
    "    # remove stopwords\n",
    "    no_stopwords = ' '.join([word for word in alphanumeric.split() if word not in STOPWORDS])\n",
    "    # stemmize\n",
    "    stemmed = ' '.join([PorterStemmer().stem(word) for word in no_stopwords.split()])\n",
    "    return stemmed\n",
    "\n",
    "def clean_page(page: dict) -> dict:\n",
    "    # apply the clean_text function to each paragraph in the page\n",
    "    cleaned_page = {}\n",
    "    # clean the URL and title\n",
    "    cleaned_page['url'], cleaned_page['title'] = page['url'], clean_text(page['title'])\n",
    "    # clean the summary paragraphs\n",
    "    if page['summary']:\n",
    "        cleaned_page['summary_ids'] = page['summary_ids']\n",
    "        cleaned_page['summary'] = [clean_text(paragraph) for paragraph in page['summary']]\n",
    "    # clean the content sections\n",
    "    cleaned_page['content'] = []\n",
    "    for section in page['content']:\n",
    "        if not section['paragraphs']:\n",
    "            continue\n",
    "        cleaned_section = {}\n",
    "        cleaned_section['type'] = section['type']\n",
    "        # clean the section title\n",
    "        cleaned_section['title'] = clean_text(section['title'])\n",
    "        # clean each paragraph in the section\n",
    "        cleaned_section['paragraphs'] = [clean_text(paragraph) for paragraph in section['paragraphs'] if clean_text(paragraph)]\n",
    "        cleaned_section['ids'] = section['ids']\n",
    "        cleaned_page['content'].append(cleaned_section)\n",
    "    return cleaned_page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the data_file\n",
    "with open('../data/raw_wiki_with_ids.json', 'r') as f:\n",
    "    data = json.load(f)\n",
    "# clean the data\n",
    "cleaned_data = [clean_page(page) for page in data]\n",
    "\n",
    "# save the cleaned data in a new file\n",
    "with open('../data/cleaned_wiki.json', 'w') as f:\n",
    "    json.dump(cleaned_data, f, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc1_3_'></a>[3 - Creating the n-grams](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open('../data/cleaned_wiki.json', 'r') as f:\n",
    "    data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_n_gram(n, text):\n",
    "    # split the text into words\n",
    "    words = text.split()\n",
    "    n_gram = []\n",
    "    # create the n-grams\n",
    "    for i in range(len(words) - n + 1):\n",
    "        # join the words to create the n-gram\n",
    "        n_gram.append(' '.join(words[i:i+n]))\n",
    "    return n_gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_n_grams(n, data):\n",
    "    n_gram_dict = {}\n",
    "    for page in data:\n",
    "        if 'summary' in page:\n",
    "            for i, paragraph in enumerate(page['summary']):\n",
    "                n_gram_par = get_n_gram(n, paragraph)\n",
    "                for n_gram in n_gram_par:\n",
    "                    if n_gram in n_gram_dict:\n",
    "                        n_gram_dict[n_gram].append(page['summary_ids'][i])\n",
    "                    else:\n",
    "                        n_gram_dict[n_gram] = [page['summary_ids'][i]]\n",
    "            \n",
    "        for section in page['content']:\n",
    "            for i, paragraph in enumerate(section['paragraphs']):\n",
    "                n_gram_par = get_n_gram(n, paragraph)\n",
    "                for n_gram in n_gram_par:\n",
    "                    if n_gram in n_gram_dict:\n",
    "                        n_gram_dict[n_gram].append(section['ids'][i])\n",
    "                    else:\n",
    "                        n_gram_dict[n_gram] = [section['ids'][i]]\n",
    "    n_gram_dict = dict(sorted(n_gram_dict.items(), key=lambda item: len(item[1]), reverse=True))\n",
    "    with open(f'../data/{n}_grams.json', 'w') as f:\n",
    "        json.dump(n_gram_dict, f)\n",
    "\n",
    "for i in range(1, 5):\n",
    "    save_n_grams(i, data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NbConvertApp] Converting notebook scrapping_cleaning.ipynb to html\n",
      "[NbConvertApp] Writing 334756 bytes to scrapping_cleaning.html\n"
     ]
    }
   ],
   "source": [
    "# convert the notebook to html\n",
    "!jupyter nbconvert --to html scrapping_cleaning.ipynb"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "wikipedia_nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
