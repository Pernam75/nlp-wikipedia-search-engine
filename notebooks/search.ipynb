{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP Final Project - Wikipedia Search Engine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 - Research"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We load all the data that we computed before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 12\u001b[0m\n\u001b[0;32m      9\u001b[0m     n_grams[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m1\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mload(f)\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m../data/2_grams.json\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m---> 12\u001b[0m     n_grams[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m2\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mjson\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m../data/3_grams.json\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m     15\u001b[0m     n_grams[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m3\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mload(f)\n",
      "File \u001b[1;32mc:\\Users\\jules\\miniconda3\\envs\\wikipedia_nlp\\Lib\\json\\__init__.py:293\u001b[0m, in \u001b[0;36mload\u001b[1;34m(fp, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[0;32m    274\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload\u001b[39m(fp, \u001b[38;5;241m*\u001b[39m, \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, object_hook\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, parse_float\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    275\u001b[0m         parse_int\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, parse_constant\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, object_pairs_hook\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw):\n\u001b[0;32m    276\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Deserialize ``fp`` (a ``.read()``-supporting file-like object containing\u001b[39;00m\n\u001b[0;32m    277\u001b[0m \u001b[38;5;124;03m    a JSON document) to a Python object.\u001b[39;00m\n\u001b[0;32m    278\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    291\u001b[0m \u001b[38;5;124;03m    kwarg; otherwise ``JSONDecoder`` is used.\u001b[39;00m\n\u001b[0;32m    292\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 293\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m loads(\u001b[43mfp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[0;32m    294\u001b[0m         \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mcls\u001b[39m, object_hook\u001b[38;5;241m=\u001b[39mobject_hook,\n\u001b[0;32m    295\u001b[0m         parse_float\u001b[38;5;241m=\u001b[39mparse_float, parse_int\u001b[38;5;241m=\u001b[39mparse_int,\n\u001b[0;32m    296\u001b[0m         parse_constant\u001b[38;5;241m=\u001b[39mparse_constant, object_pairs_hook\u001b[38;5;241m=\u001b[39mobject_pairs_hook, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw)\n",
      "\u001b[1;31mMemoryError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import json\n",
    "# this two functions are the same as in the cleaning part so we apply exactly the same preprocess to the query as to the paragraphs\n",
    "from utils import clean_text, get_n_gram\n",
    "import re\n",
    "\n",
    "n_grams = {}\n",
    "\n",
    "with open('../data/1_grams.json') as f:\n",
    "    n_grams['1'] = json.load(f)\n",
    "\n",
    "with open('../data/2_grams.json') as f:\n",
    "    n_grams['2'] = json.load(f)\n",
    "\n",
    "with open('../data/3_grams.json') as f:\n",
    "    n_grams['3'] = json.load(f)\n",
    "\n",
    "with open('../data/4_grams.json') as f:\n",
    "    n_grams['4'] = json.load(f)\n",
    "\n",
    "with open('../data/raw_wiki_with_ids.json') as f:\n",
    "    wiki = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need a function that can find a paragraph in the raw database from a given id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Python is a high-level, general-purpose programming language. Its design philosophy emphasizes code readability with the use of significant indentation.[31]\\n',\n",
       " 'https://en.wikipedia.org/wiki/Python_(programming_language)')"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def find_paragraph(id_paragraph):\n",
    "    \"\"\"\n",
    "    This function returns the paragraph of the wiki that has the id given as parameter\n",
    "\n",
    "    Args:\n",
    "    id_paragraph: int, id of the paragraph to be found\n",
    "    \"\"\"\n",
    "    for page in wiki:\n",
    "        for section in page['content']:\n",
    "            if id_paragraph in section['ids']:\n",
    "                text = section['paragraphs'][section['ids'].index(id_paragraph)]\n",
    "                # formatted section title\n",
    "                section_title = section['title'].replace(' ', '_')\n",
    "                # remove references in square brackets\n",
    "                section_title = re.sub(r'\\[.*?\\]', '', section_title)\n",
    "                url = page['url'] + '#' + section_title\n",
    "                return text, url\n",
    "        if 'summary_ids' in page:\n",
    "            if id_paragraph in page['summary_ids']:\n",
    "                text = page['summary'][page['summary_ids'].index(id_paragraph)]\n",
    "                url = page['url']\n",
    "                return text, url\n",
    "            \n",
    "find_paragraph(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_ngram_frequency(query_ngrams, db_ngrams):\n",
    "    \"\"\"\n",
    "    This function returns the frequency of the n-grams in the query in the paragraphs\n",
    "\n",
    "    Args:\n",
    "    query_ngrams: list of str, n-grams of the query\n",
    "    db_ngrams: dict, n-grams of the paragraphs\n",
    "    \"\"\"\n",
    "    n_grams_frequency = {n_gram: {} for n_gram in query_ngrams}\n",
    "    for word in query_ngrams:\n",
    "        # if the word is not in the db_ngrams we skip it\n",
    "        if word in db_ngrams:\n",
    "            word_par = db_ngrams[word]\n",
    "            # we iterate over the paragraphs where the word is present\n",
    "            for par in word_par:\n",
    "                # if the paragraph is already in the n_grams_frequency we increment the frequency\n",
    "                if par in n_grams_frequency[word]:\n",
    "                    n_grams_frequency[word][par] += 1\n",
    "                # if not we add the paragraph to the dictionary\n",
    "                else:\n",
    "                    n_grams_frequency[word][par] = 1\n",
    "    # we return the dictionary with the frequency of the n-grams in the documents sorted by frequency\n",
    "    return {n_gram: {k: v for k, v in sorted(par.items(), key=lambda item: item[1], reverse=True)} for n_gram, par in n_grams_frequency.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will try to find how many times a paragraph has the n-grams in its text. For this part, we do not care about how many times the n-gram appears in the paragraph, we just want to know if it appears at least once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_unique_ngram(ngram_freq):\n",
    "    \"\"\"\n",
    "    This function counts the number of unique n-grams coming from the query in the paragraphs\n",
    "\n",
    "    Args:\n",
    "    ngram_freq: dict, frequency of the n-grams in the paragraphs\n",
    "    \"\"\"\n",
    "    par_unique_ngram = {}\n",
    "    # count how many unique n_grams are in each paragraph\n",
    "    for _, paragraphs in ngram_freq.items():\n",
    "        for par in paragraphs:\n",
    "            added = False\n",
    "            # if the paragraph is already in the dictionary we increment the count\n",
    "            if par in par_unique_ngram:\n",
    "                # we increment the count of the paragraph only if the n-gram has not already been detected in the paragraph\n",
    "                if not added:\n",
    "                    par_unique_ngram[par] += 1\n",
    "                    added = True\n",
    "            else:\n",
    "                par_unique_ngram[par] = 1\n",
    "\n",
    "    # sort the paragraphs by the number of unique n_grams\n",
    "    return {k: v for k, v in sorted(par_unique_ngram.items(), key=lambda item: item[1], reverse=True)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will compute a score for each paragraph and for each n of n-grams. The score will be the number of times the n-gram appears in the paragraph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_scores(ngrams_query, ngrams_db, n):\n",
    "    \"\"\"\n",
    "    This function returns the scores of the paragraphs based on the each n-grams\n",
    "\n",
    "    Args:\n",
    "    ngrams_query: dict, n-grams of the query\n",
    "    ngrams_db: dict, n-grams of the paragraphs\n",
    "    n: int, max n-gram to be considered\n",
    "    \"\"\"\n",
    "    scores = {str(n): {} for n in range(1, n+1)}\n",
    "    n_gram_frequencies = {str(n): {} for n in range(1, n+1)}\n",
    "    for n_gram in ngrams_query:\n",
    "        n_gram_frequencies[n_gram] = count_ngram_frequency(ngrams_query[n_gram], ngrams_db[n_gram])\n",
    "        scores[n_gram] = get_unique_ngram(n_gram_frequencies[n_gram])\n",
    "\n",
    "    return scores, n_gram_frequencies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then combine the scores of each n-gram to get a final score for the paragraph. We multiply the score of each n-gram by a weight that depends on the length of the n-gram. \n",
    "\n",
    "The bigger n is, the more important the n-gram is as it means that the looked words are closer to each other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_scores(scores, n_gram_frequencies, weights):\n",
    "    \"\"\"\n",
    "    This function returns the combined scores of the paragraphs based on the scores of the n-grams\n",
    "\n",
    "    Args:\n",
    "    scores: dict, scores of the paragraphs based on the n-grams\n",
    "    n_gram_frequencies: dict, frequency of the n-grams in the paragraphs\n",
    "    weights: dict, weights of the n-grams\n",
    "    \"\"\"\n",
    "    # the score of the paragraph is the sum of the scores of the n_grams multiplied by the weight of the n_gram\n",
    "    combined_scores = {}\n",
    "    for n_gram in scores:\n",
    "        for par, score in scores[n_gram].items():\n",
    "            if par in combined_scores:\n",
    "                combined_scores[par] += score * weights[n_gram] + n_gram_frequencies[n_gram][par] if par in n_gram_frequencies[n_gram] else score * weights[n_gram]\n",
    "            else:\n",
    "                combined_scores[par] = score * weights[n_gram] + n_gram_frequencies[n_gram][par] if par in n_gram_frequencies[n_gram] else score * weights[n_gram]\n",
    "\n",
    "    return {k: v for k, v in sorted(combined_scores.items(), key=lambda item: item[1], reverse=True)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this n represents the maximum n-gram we want to use\n",
    "n = 4\n",
    "max_returned_paragraphs = 5\n",
    "# these are the weights of the n-grams.\n",
    "weights = {\n",
    "    '1': 1,\n",
    "    '2': 10,\n",
    "    '3': 100,\n",
    "    '4': 1000\n",
    "}\n",
    "query = 'When was the first computer invented?'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search(query, n_grams, n, weights):\n",
    "    \"\"\"\n",
    "    This function returns the paragraphs ids that are the most relevant to the query and their scores\n",
    "\n",
    "    Args:\n",
    "    query: str, query to be searched\n",
    "    n_grams: dict, n-grams of the paragraphs\n",
    "    n: int, max n-gram to be considered\n",
    "    \"\"\"\n",
    "    cleaned_query = clean_text(query)\n",
    "    ngrams_query = {str(n): get_n_gram(n, cleaned_query) for n in range(1, n+1)}\n",
    "    scores, n_gram_frequencies = get_scores(ngrams_query, n_grams, n)\n",
    "    ranking = combine_scores(scores, n_gram_frequencies, weights)\n",
    "    return ranking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'json' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m../data/sorted_links.json\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m----> 2\u001b[0m     links \u001b[38;5;241m=\u001b[39m \u001b[43mjson\u001b[49m\u001b[38;5;241m.\u001b[39mload(f)\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mlink_score\u001b[39m(link):\n\u001b[0;32m      4\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;124;03m    This function returns the score of the link\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \n\u001b[0;32m      7\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;124;03m    link: str, link to be scored\u001b[39;00m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'json' is not defined"
     ]
    }
   ],
   "source": [
    "with open('../data/sorted_links.json') as f:\n",
    "    links = json.load(f)\n",
    "def link_score(link):\n",
    "    \"\"\"\n",
    "    This function returns the score of the link\n",
    "\n",
    "    Args:\n",
    "    link: str, link to be scored\n",
    "    \"\"\"\n",
    "    # the score is the number of citations of the link divided by 10\n",
    "    return links[link] / 10\n",
    "\n",
    "link_score('https://en.wikipedia.org/wiki/Computer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wilhelm Schickard designed and constructed the first working mechanical calculator in 1623.[18] In 1673, Gottfried Leibniz demonstrated a digital mechanical calculator, called the Stepped Reckoner.[19] Leibniz may be considered the first computer scientist and information theorist, because of various reasons, including the fact that he documented the binary number system. In 1820, Thomas de Colmar launched the mechanical calculator industry[note 1] when he invented his simplified arithmometer, the first calculating machine strong enough and reliable enough to be used daily in an office environment. Charles Babbage started the design of the first automatic mechanical calculator, his Difference Engine, in 1822, which eventually gave him the idea of the first programmable mechanical calculator, his Analytical Engine.[20] He started developing this machine in 1834, and \"in less than two years, he had sketched out many of the salient features of the modern computer\".[21] \"A crucial step was the adoption of a punched card system derived from the Jacquard loom\"[21] making it infinitely programmable.[note 2] In 1843, during the translation of a French article on the Analytical Engine, Ada Lovelace wrote, in one of the many notes she included, an algorithm to compute the Bernoulli numbers, which is considered to be the first published algorithm ever specifically tailored for implementation on a computer.[22] Around 1885, Herman Hollerith invented the tabulator, which used punched cards to process statistical information; eventually his company became part of IBM. Following Babbage, although unaware of his earlier work, Percy Ludgate in 1909 published[23] the 2nd of the only two designs for mechanical analytical engines in history. In 1914, the Spanish engineer Leonardo Torres Quevedo published his Essays on Automatics,[24] and designed, inspired by Babbage, a theoretical electromechanical calculating machine which was to be controlled by a read-only program. The paper also introduced the idea of floating-point arithmetic.[25][26] In 1920, to celebrate the 100th anniversary of the invention of the arithmometer, Torres presented in Paris the Electromechanical Arithmometer, a prototype that demonstrated the feasibility of an electromechanical analytical engine,[27] on which commands could be typed and the results printed automatically.[28] In 1937, one hundred years after Babbage's impossible dream, Howard Aiken convinced IBM, which was making all kinds of punched card equipment and was also in the calculator business[29] to develop his giant programmable calculator, the ASCC/Harvard Mark I, based on Babbage's Analytical Engine, which itself used cards and a central computing unit. When the machine was finished, some hailed it as \"Babbage's dream come true\".[30]\n",
      "\n",
      "https://en.wikipedia.org/wiki/Computer_science\n",
      "---------------------------------\n",
      "Wilhelm Schickard designed and constructed the first working mechanical calculator in 1623.[18] In 1673, Gottfried Leibniz demonstrated a digital mechanical calculator, called the Stepped Reckoner.[19] Leibniz may be considered the first computer scientist and information theorist, because of various reasons, including the fact that he documented the binary number system. In 1820, Thomas de Colmar launched the mechanical calculator industry[note 1] when he invented his simplified arithmometer, the first calculating machine strong enough and reliable enough to be used daily in an office environment. Charles Babbage started the design of the first automatic mechanical calculator, his Difference Engine, in 1822, which eventually gave him the idea of the first programmable mechanical calculator, his Analytical Engine.[20] He started developing this machine in 1834, and \"in less than two years, he had sketched out many of the salient features of the modern computer\".[21] \"A crucial step was the adoption of a punched card system derived from the Jacquard loom\"[21] making it infinitely programmable.[note 2] In 1843, during the translation of a French article on the Analytical Engine, Ada Lovelace wrote, in one of the many notes she included, an algorithm to compute the Bernoulli numbers, which is considered to be the first published algorithm ever specifically tailored for implementation on a computer.[22] Around 1885, Herman Hollerith invented the tabulator, which used punched cards to process statistical information; eventually his company became part of IBM. Following Babbage, although unaware of his earlier work, Percy Ludgate in 1909 published[23] the 2nd of the only two designs for mechanical analytical engines in history. In 1914, the Spanish engineer Leonardo Torres Quevedo published his Essays on Automatics,[24] and designed, inspired by Babbage, a theoretical electromechanical calculating machine which was to be controlled by a read-only program. The paper also introduced the idea of floating-point arithmetic.[25][26] In 1920, to celebrate the 100th anniversary of the invention of the arithmometer, Torres presented in Paris the Electromechanical Arithmometer, a prototype that demonstrated the feasibility of an electromechanical analytical engine,[27] on which commands could be typed and the results printed automatically.[28] In 1937, one hundred years after Babbage's impossible dream, Howard Aiken convinced IBM, which was making all kinds of punched card equipment and was also in the calculator business[29] to develop his giant programmable calculator, the ASCC/Harvard Mark I, based on Babbage's Analytical Engine, which itself used cards and a central computing unit. When the machine was finished, some hailed it as \"Babbage's dream come true\".[30]\n",
      "\n",
      "https://en.wikipedia.org/wiki/Computer_science#History\n",
      "---------------------------------\n",
      "John Whitney developed the first computer-generated art in the early 1960s by utilizing mathematical operations to create art.[6] In 1963, Ivan Sutherland invented the first user interactive computer-graphics interface known as Sketchpad.[7]\n",
      "Between 1974 and 1977, Salvador Dalí created two big canvases of Gala Contemplating the Mediterranean Sea which at a distance of 20 meters is transformed into the portrait of Abraham Lincoln (Homage to Rothko)[8] and prints of Lincoln in Dalivision based on a portrait of Abraham Lincoln processed on a computer by Leon Harmon published in \"The Recognition of Faces\".[9]\n",
      "The technique is similar to what later became known as photographic mosaics.\n",
      "\n",
      "https://en.wikipedia.org/wiki/Digital_art#History\n",
      "---------------------------------\n",
      "David Wheeler, who earned the world's first Computer Science PhD working on the project, is credited with inventing the concept of a subroutine. Users wrote programs that called a routine by jumping to the start of the subroutine with the return address (i.e. the location-plus-one of the jump itself) in the accumulator (a Wheeler Jump). By convention the subroutine expected this, and the first thing it did was to modify its concluding jump instruction to that return address. Multiple and nested subroutines could be called so long as the user knew the length of each one in order to calculate the location to jump to; recursive calls were forbidden. The user then copied the code for the subroutine from a master tape onto their own tape following the end of their own program. (However, Alan Turing discussed subroutines in a paper of 1945 on design proposals for the NPL ACE, going so far as to invent the concept of a return-address stack, which would have allowed recursion.[20])\n",
      "\n",
      "https://en.wikipedia.org/wiki/EDSAC#Programming_technique\n",
      "---------------------------------\n",
      "Also, in 1966, Ivan Sutherland continued to innovate at MIT when he invented the first computer-controlled head-mounted display (HMD). It displayed two separate wireframe images, one for each eye. This allowed the viewer to see the computer scene in stereoscopic 3D. The heavy hardware required for supporting the display and tracker was called the Sword of Damocles because of the potential danger if it were to fall upon the wearer. After receiving his Ph.D. from MIT, Sutherland became Director of Information Processing at ARPA (Advanced Research Projects Agency), and later became a professor at Harvard. In 1967 Sutherland was recruited by Evans to join the computer science program at the University of Utah – a development which would turn that department into one of the most important research centers in graphics for nearly a decade thereafter, eventually producing some of the most important pioneers in the field. There Sutherland perfected his HMD; twenty years later, NASA would re-discover his techniques in their virtual reality research. At Utah, Sutherland and Evans were highly sought after consultants by large companies, but they were frustrated at the lack of graphics hardware available at the time, so they started formulating a plan to start their own company.\n",
      "\n",
      "https://en.wikipedia.org/wiki/Computer_graphics#1960s\n",
      "---------------------------------\n"
     ]
    }
   ],
   "source": [
    "ranking = search(query, n_grams, n, weights)\n",
    "# we get the top paragraphs\n",
    "ranking = list(ranking.keys())[:max_returned_paragraphs]\n",
    "for paragraph in ranking:\n",
    "    text, url = find_paragraph(paragraph)\n",
    "    print(text + '\\n' + url)\n",
    "    print('---------------------------------')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We remarked at the end of the project that we have duplicates in the database.\n",
    "\n",
    "The reason of these duplicates is that in the scrapping phase, we took care to exclude the links of the form https://en.wikipedia.org/wiki/Title once they have been visited. However, we did not take care of the links of the form https://en.wikipedia.org/wiki/Title#section that are considered as different links by the scrapping algorithm.\n",
    "\n",
    "As the scrapping phase is very time consuming, we decided to keep the duplicates and to not recompute the scrapping phase. Moreover, the duplicates must not be very frequent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NbConvertApp] Converting notebook search.ipynb to html\n",
      "[NbConvertApp] Writing 314368 bytes to search.html\n"
     ]
    }
   ],
   "source": [
    "!jupyter nbconvert --to html search.ipynb"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "wikipedia_nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
